{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6400 images belonging to 4 classes.\n",
      "Label 0: torch.Size([175, 3, 128, 128])\n",
      "Label 1: torch.Size([15, 3, 128, 128])\n",
      "Label 2: torch.Size([620, 3, 128, 128])\n",
      "Label 3: torch.Size([470, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    'C:\\\\Users\\\\walter\\\\OneDrive\\\\桌面\\\\收集\\\\大學nn專題\\\\nn實作\\\\阿茲海默症預測\\\\archive\\\\Dataset',\n",
    "    target_size=(128, 128),#保持\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data, labels = next(data_generator)\n",
    "for _ in range(len(data_generator) - 1):\n",
    "    imgs, lbls = next(data_generator)\n",
    "    data = np.append(data, imgs, axis=0)\n",
    "    labels = np.append(labels, lbls, axis=0)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "train_data = train_data.permute(0, 3, 1, 2)  # Change data shape from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "test_data = test_data.permute(0, 3, 1, 2)  # Change data shape from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "\n",
    "# 獲取所有唯一的label\n",
    "unique_labels = torch.unique(test_labels)\n",
    "\n",
    "# 創建一個字典來保存分類後的數據\n",
    "grouped_test_data = {label.item(): [] for label in unique_labels}\n",
    "\n",
    "# 遍歷所有label，將對應的data加入字典中的相應列表\n",
    "for label in unique_labels:\n",
    "    grouped_test_data[label.item()] = test_data[test_labels == label]\n",
    "    print(f\"Label {label.item()}: {grouped_test_data[label.item()].size()}\")\n",
    "label_to_name = {  \n",
    "    0: 'Non_Demented',  \n",
    "    1: 'Very_Mild_Demented',  \n",
    "    2: 'Mild_Demented',  \n",
    "    3: 'Moderate_Demented'  \n",
    "}\n",
    "\n",
    "# grouped_data 現在包含了分類後的數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "weights tensor([ 1.0704, 11.7371,  0.3527,  0.4427], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-2c82a146a6c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[0mtoptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#weight0.0005 原:0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t7\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-2c82a146a6c6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_data, train_labels, test_data, test_labels, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mtrain_l_sum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "CHANNELS = 3\n",
    "IMG_DIM = 128\n",
    "NGPU = 1\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "from torch import nn,optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            #in_channels ,out_channels, kernel_size\n",
    "            nn.Conv2d(CHANNELS,3,5),\n",
    "            nn.Tanh(),\n",
    "            #kernel_size, stride\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(3,3,5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(3,3,6),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(3*12*12,360),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(360,120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120,4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        feature=self.conv(img)\n",
    "       # print(\"Conv output shape: \", feature.shape)  # 打印卷積層輸出的形狀\n",
    "        \n",
    "        output=self.fc(feature.view(img.shape[0],-1))\n",
    "        output=self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_iter,net,device=None):\n",
    "    if device is None and isinstance(net,torch.nn.Module):\n",
    "        device=list(net.parameters())[0].device\n",
    "    acc_sum,n=0.0,0\n",
    "    for x,y in data_iter:\n",
    "        if isinstance(net,torch.nn.Module):\n",
    "            #評估模式，關閉dropout\n",
    "            net.eval()\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            acc_sum+=(net(x).argmax(dim=1)==y).float().sum().cpu().item()\n",
    "            net.train()\n",
    "        else:\n",
    "            if('is_training' in net.__code__.co_varnames):\n",
    "                acc_sum+=(net(x,is_training=False).argmax(dim=1)==y).float().sum().cpu().item()\n",
    "            else:\n",
    "                acc_sum+=(net(x).argmax(dim=1)==y).float().sum().cpu().item()\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "\n",
    "def train(net,train_data,train_labels,test_data,test_labels,batch_size,optimizer,device,num_epochs):\n",
    "    \n",
    "    # 學習率調度器\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.99, verbose=True)\n",
    "\n",
    "    #train_iter-測試樣本劃分為最小批的結果\n",
    "    # 初始化 accuracy_dict，為每個label名稱創建一個空列表\n",
    "    accuracy_dict = {name: [] for name in label_to_name.values()}\n",
    "    ls=[]\n",
    "    net=net.to(device)\n",
    "    print(\"training on \",device)\n",
    "\n",
    "    a=1/grouped_test_data[0].size(0)\n",
    "    b=1/grouped_test_data[1].size(0)\n",
    "    c=1/grouped_test_data[2].size(0)\n",
    "    d=1/grouped_test_data[3].size(0)\n",
    "    weights = torch.tensor([a/a+b+c+d, b/a+b+c+d, c/a+b+c+d,d/a+b+c+d]).to('cuda')   # 這裡的數字可以根據需要調整\n",
    "\n",
    "  # 這裡的數字可以根據需要調整\n",
    "    print(\"weights\",weights)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,batch_count=0.0,0.0,0,0\n",
    "        start=time.time()\n",
    "        dataset=torch.utils.data.TensorDataset(train_data,train_labels)#將標籤和特徵打包\n",
    "        #DataLoader本質是一迭代器，批次處理data\n",
    "        train_iter=torch.utils.data.DataLoader(dataset,batch_size,shuffle=True)#shuffle=True為打亂dataset\n",
    "\n",
    "        for data,labels in train_iter:\n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            y_hat=net(data)\n",
    "            l=loss(y_hat,labels)\n",
    "            #梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            ls.append(l)\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==labels).sum().cpu().item()\n",
    "            n+=labels.shape[0]\n",
    "            batch_count+=1\n",
    "        \n",
    "        \n",
    "        print('epoch ',epoch+1,',loss ',train_l_sum/batch_count,', train acc',train_acc_sum/n)\n",
    "        # 遍歷每個label\n",
    "        for label in grouped_test_data.keys():\n",
    "            # 獲取對應的 data 和 label\n",
    "            data = grouped_test_data[label]\n",
    "            labels = torch.tensor([label] * len(data), dtype=torch.long)  # 為每個樣本生成相應的 label\n",
    "\n",
    "            # 創建 TensorDataset 和 DataLoader\n",
    "            dataset = TensorDataset(data, labels)\n",
    "            data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # 計算該label的準確度\n",
    "            test_acc = evaluate_accuracy(data_loader, net)\n",
    "            # 使用label_to_name將label轉換為名稱，並將test_acc儲存到字典中\n",
    "            name = label_to_name[label]\n",
    "            accuracy_dict[name].append(test_acc)\n",
    "            print('label',name,\"acc\",test_acc)\n",
    "\n",
    "        print('time',time.time()-start,'sec','net par')\n",
    "        #scheduler.step(test_acc)\n",
    "\n",
    "    \n",
    "        ''' \n",
    "        for name, param in net.conv.named_parameters():\n",
    "            print(f'{name} grad: {param.grad}')\n",
    "        '''\n",
    "\n",
    "    x=np.linspace(0,num_epochs,num=num_epochs)\n",
    "    for label in grouped_test_data.keys():\n",
    "        name = label_to_name[label]  # 将label转换为对应的名称\n",
    "        plt.plot(x, accuracy_dict[name], label=name)    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('test_acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    x_2=np.linspace(0,num_epochs,len(ls))\n",
    "    plt.plot(x_2,ls)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "num_epochs=500\n",
    "net=LeNet()\n",
    "# 初始化网络参数\n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "toptimizer = optim.Adam(net.parameters(), lr=0.0005)#weight0.0005 原:0.001\n",
    "train(net,train_data=train_data,train_labels=train_labels,test_data=test_data,test_labels=test_labels,batch_size=40,optimizer=toptimizer,device='cuda',num_epochs=num_epochs)\n",
    "print(\"t7\")\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
