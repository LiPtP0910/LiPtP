{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6400 images belonging to 4 classes.\n",
      "Label 0: torch.Size([172, 3, 128, 128])\n",
      "Label 1: torch.Size([13, 3, 128, 128])\n",
      "Label 2: torch.Size([640, 3, 128, 128])\n",
      "Label 3: torch.Size([455, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    'C:\\\\Users\\\\walter\\\\OneDrive\\\\桌面\\\\收集\\\\大學nn專題\\\\nn實作\\\\阿茲海默症預測\\\\archive\\\\Dataset',\n",
    "    target_size=(128, 128),#保持\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data, labels = next(data_generator)\n",
    "for _ in range(len(data_generator) - 1):\n",
    "    imgs, lbls = next(data_generator)\n",
    "    data = np.append(data, imgs, axis=0)\n",
    "    labels = np.append(labels, lbls, axis=0)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "train_data = train_data.permute(0, 3, 1, 2)  # Change data shape from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "test_data = test_data.permute(0, 3, 1, 2)  # Change data shape from (batch, height, width, channels) to (batch, channels, height, width)\n",
    "\n",
    "# 獲取所有唯一的label\n",
    "unique_labels = torch.unique(test_labels)\n",
    "\n",
    "# 創建一個字典來保存分類後的數據\n",
    "grouped_test_data = {label.item(): [] for label in unique_labels}\n",
    "\n",
    "# 遍歷所有label，將對應的data加入字典中的相應列表\n",
    "for label in unique_labels:\n",
    "    grouped_test_data[label.item()] = test_data[test_labels == label]\n",
    "    print(f\"Label {label.item()}: {grouped_test_data[label.item()].size()}\")\n",
    "label_to_name = {  \n",
    "    0: 'Non_Demented',  \n",
    "    1: 'Very_Mild_Demented',  \n",
    "    2: 'Mild_Demented',  \n",
    "    3: 'Moderate_Demented'  \n",
    "}\n",
    "\n",
    "# grouped_data 現在包含了分類後的數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch  1 ,loss  1.3093340443447232 , train acc 0.4357421875\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.9953125\n",
      "label Moderate_Demented mistake 1.0\n",
      "time 11.66906476020813 sec net par\n",
      "epoch  2 ,loss  1.248148676007986 , train acc 0.493359375\n",
      "label Non_Demented mistake 0.5755813953488372\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch     6: reducing learning rate of group 0 to 9.9000e-04.\n",
      "label Mild_Demented mistake 0.14531249999999996\n",
      "label Moderate_Demented mistake 0.9142857142857143\n",
      "time 11.6793851852417 sec net par\n",
      "epoch  3 ,loss  1.225888422690332 , train acc 0.5125\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 1.0\n",
      "Epoch    11: reducing learning rate of group 0 to 9.8010e-04.\n",
      "label Moderate_Demented mistake 1.0\n",
      "time 11.697445631027222 sec net par\n",
      "epoch  4 ,loss  1.2139278966933489 , train acc 0.5259765625\n",
      "label Non_Demented mistake 0.6569767441860466\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.15312499999999996\n",
      "label Moderate_Demented mistake 0.7626373626373626\n",
      "Epoch    16: reducing learning rate of group 0 to 9.7030e-04.\n",
      "time 11.687933444976807 sec net par\n",
      "epoch  5 ,loss  1.1849097898229957 , train acc 0.5171875\n",
      "label Non_Demented mistake 0.563953488372093\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.24375000000000002\n",
      "label Moderate_Demented mistake 0.6307692307692307\n",
      "time 11.739377975463867 sec net par\n",
      "epoch  6 ,loss  1.13756712526083 , train acc 0.539453125\n",
      "label Non_Demented mistake 1.0\n",
      "Epoch    21: reducing learning rate of group 0 to 9.6060e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0\n",
      "label Moderate_Demented mistake 1.0\n",
      "time 11.806761264801025 sec net par\n",
      "epoch  7 ,loss  1.1475483747199178 , train acc 0.5380859375\n",
      "label Non_Demented mistake 1.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch    26: reducing learning rate of group 0 to 9.5099e-04.\n",
      "label Mild_Demented mistake 0.0\n",
      "label Moderate_Demented mistake 0.9934065934065934\n",
      "time 11.78714108467102 sec net par\n",
      "epoch  8 ,loss  1.1047412296757102 , train acc 0.5693359375\n",
      "label Non_Demented mistake 0.16279069767441856\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.509375\n",
      "Epoch    31: reducing learning rate of group 0 to 9.4148e-04.\n",
      "label Moderate_Demented mistake 0.6571428571428571\n",
      "time 11.740694046020508 sec net par\n",
      "epoch  9 ,loss  1.1067350571975112 , train acc 0.58203125\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.915625\n",
      "label Moderate_Demented mistake 0.8769230769230769\n",
      "Epoch    36: reducing learning rate of group 0 to 9.3207e-04.\n",
      "time 11.737253189086914 sec net par\n",
      "epoch  10 ,loss  1.0998818539083004 , train acc 0.5732421875\n",
      "label Non_Demented mistake 0.9825581395348837\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0015625000000000222\n",
      "label Moderate_Demented mistake 0.9736263736263736\n",
      "time 11.724255800247192 sec net par\n",
      "epoch  11 ,loss  1.0785683714784682 , train acc 0.592578125\n",
      "label Non_Demented mistake 0.9593023255813954\n",
      "Epoch    41: reducing learning rate of group 0 to 9.2274e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.9472527472527472\n",
      "time 11.722986936569214 sec net par\n",
      "epoch  12 ,loss  1.089190574362874 , train acc 0.5775390625\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch    46: reducing learning rate of group 0 to 9.1352e-04.\n",
      "label Mild_Demented mistake 0.7703125\n",
      "label Moderate_Demented mistake 0.8\n",
      "time 11.71701955795288 sec net par\n",
      "epoch  13 ,loss  1.0554320449009538 , train acc 0.600390625\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.44062500000000004\n",
      "Epoch    51: reducing learning rate of group 0 to 9.0438e-04.\n",
      "label Moderate_Demented mistake 0.6857142857142857\n",
      "time 11.71191954612732 sec net par\n",
      "epoch  14 ,loss  1.0531438435427845 , train acc 0.6138671875\n",
      "label Non_Demented mistake 0.6104651162790697\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.09687500000000004\n",
      "label Moderate_Demented mistake 0.8351648351648352\n",
      "Epoch    56: reducing learning rate of group 0 to 8.9534e-04.\n",
      "time 11.728649377822876 sec net par\n",
      "epoch  15 ,loss  1.0354225365445018 , train acc 0.64453125\n",
      "label Non_Demented mistake 0.9186046511627907\n",
      "label Very_Mild_Demented mistake 0.9230769230769231\n",
      "label Mild_Demented mistake 0.025000000000000022\n",
      "label Moderate_Demented mistake 0.9560439560439561\n",
      "time 11.733339786529541 sec net par\n",
      "epoch  16 ,loss  1.0468929330818355 , train acc 0.6275390625\n",
      "label Non_Demented mistake 0.3023255813953488\n",
      "Epoch    61: reducing learning rate of group 0 to 8.8638e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.265625\n",
      "label Moderate_Demented mistake 0.49230769230769234\n",
      "time 11.855375289916992 sec net par\n",
      "epoch  17 ,loss  1.0177194378338754 , train acc 0.6533203125\n",
      "label Non_Demented mistake 0.42441860465116277\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch    66: reducing learning rate of group 0 to 8.7752e-04.\n",
      "label Mild_Demented mistake 0.17812499999999998\n",
      "label Moderate_Demented mistake 0.6637362637362637\n",
      "time 11.820711612701416 sec net par\n",
      "epoch  18 ,loss  1.0202526967041194 , train acc 0.6453125\n",
      "label Non_Demented mistake 0.12790697674418605\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.4390625\n",
      "Epoch    71: reducing learning rate of group 0 to 8.6875e-04.\n",
      "label Moderate_Demented mistake 0.356043956043956\n",
      "time 11.769241333007812 sec net par\n",
      "epoch  19 ,loss  0.9868658762425184 , train acc 0.6947265625\n",
      "label Non_Demented mistake 0.13372093023255816\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.2421875\n",
      "label Moderate_Demented mistake 0.5538461538461539\n",
      "Epoch    76: reducing learning rate of group 0 to 8.6006e-04.\n",
      "time 11.743334770202637 sec net par\n",
      "epoch  20 ,loss  0.9672031118534505 , train acc 0.71328125\n",
      "label Non_Demented mistake 0.34302325581395354\n",
      "label Very_Mild_Demented mistake 0.8461538461538461\n",
      "label Mild_Demented mistake 0.1328125\n",
      "label Moderate_Demented mistake 0.6417582417582417\n",
      "time 11.7138192653656 sec net par\n",
      "epoch  21 ,loss  0.9776105070486665 , train acc 0.718359375\n",
      "label Non_Demented mistake 0.06395348837209303\n",
      "Epoch    81: reducing learning rate of group 0 to 8.5146e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.3453125\n",
      "label Moderate_Demented mistake 0.37142857142857144\n",
      "time 11.73010540008545 sec net par\n",
      "epoch  22 ,loss  0.9422104898840189 , train acc 0.7521484375\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch    86: reducing learning rate of group 0 to 8.4294e-04.\n",
      "label Mild_Demented mistake 0.8234375\n",
      "label Moderate_Demented mistake 0.46153846153846156\n",
      "time 11.719827890396118 sec net par\n",
      "epoch  23 ,loss  0.9195478758774698 , train acc 0.7873046875\n",
      "label Non_Demented mistake 0.03488372093023251\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.459375\n",
      "Epoch    91: reducing learning rate of group 0 to 8.3451e-04.\n",
      "label Moderate_Demented mistake 0.22417582417582416\n",
      "time 11.707764387130737 sec net par\n",
      "epoch  24 ,loss  0.9061183482408524 , train acc 0.8169921875\n",
      "label Non_Demented mistake 0.5348837209302326\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.09843749999999996\n",
      "label Moderate_Demented mistake 0.523076923076923\n",
      "Epoch    96: reducing learning rate of group 0 to 8.2617e-04.\n",
      "time 11.714876890182495 sec net par\n",
      "epoch  25 ,loss  0.9162053312174976 , train acc 0.8107421875\n",
      "label Non_Demented mistake 0.05813953488372092\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.65\n",
      "label Moderate_Demented mistake 0.22637362637362635\n",
      "time 11.713844060897827 sec net par\n",
      "epoch  26 ,loss  0.8914442453533411 , train acc 0.8435546875\n",
      "label Non_Demented mistake 0.0\n",
      "Epoch   101: reducing learning rate of group 0 to 8.1791e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.7578125\n",
      "label Moderate_Demented mistake 0.6329670329670329\n",
      "time 11.710355758666992 sec net par\n",
      "epoch  27 ,loss  0.880732923746109 , train acc 0.86015625\n",
      "label Non_Demented mistake 0.06976744186046513\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   106: reducing learning rate of group 0 to 8.0973e-04.\n",
      "label Mild_Demented mistake 0.17656249999999996\n",
      "label Moderate_Demented mistake 0.21978021978021978\n",
      "time 11.712273120880127 sec net par\n",
      "epoch  28 ,loss  0.8729063207283616 , train acc 0.8669921875\n",
      "label Non_Demented mistake 0.2383720930232558\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.059374999999999956\n",
      "Epoch   111: reducing learning rate of group 0 to 8.0163e-04.\n",
      "label Moderate_Demented mistake 0.523076923076923\n",
      "time 11.718952417373657 sec net par\n",
      "epoch  29 ,loss  0.8494322299957275 , train acc 0.898046875\n",
      "label Non_Demented mistake 0.11046511627906974\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.19062500000000004\n",
      "label Moderate_Demented mistake 0.15384615384615385\n",
      "Epoch   116: reducing learning rate of group 0 to 7.9361e-04.\n",
      "time 11.725301265716553 sec net par\n",
      "epoch  30 ,loss  0.8435158808715641 , train acc 0.905078125\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.09218749999999998\n",
      "label Moderate_Demented mistake 0.24395604395604398\n",
      "time 11.726389169692993 sec net par\n",
      "epoch  31 ,loss  0.829711651429534 , train acc 0.924609375\n",
      "label Non_Demented mistake 0.11627906976744184\n",
      "Epoch   121: reducing learning rate of group 0 to 7.8568e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.07343750000000004\n",
      "label Moderate_Demented mistake 0.10329670329670326\n",
      "time 11.735848426818848 sec net par\n",
      "epoch  32 ,loss  0.8283810196444392 , train acc 0.930078125\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   126: reducing learning rate of group 0 to 7.7782e-04.\n",
      "label Mild_Demented mistake 0.3140625\n",
      "label Moderate_Demented mistake 0.13626373626373622\n",
      "time 11.755208730697632 sec net par\n",
      "epoch  33 ,loss  0.8193901060149074 , train acc 0.9431640625\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.15937500000000004\n",
      "Epoch   131: reducing learning rate of group 0 to 7.7004e-04.\n",
      "label Moderate_Demented mistake 0.15164835164835166\n",
      "time 11.737837553024292 sec net par\n",
      "epoch  34 ,loss  0.8154345559887588 , train acc 0.9474609375\n",
      "label Non_Demented mistake 0.22093023255813948\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.3120879120879121\n",
      "Epoch   136: reducing learning rate of group 0 to 7.6234e-04.\n",
      "time 11.738686323165894 sec net par\n",
      "epoch  35 ,loss  0.8135757627896965 , train acc 0.950390625\n",
      "label Non_Demented mistake 0.05232558139534882\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.16093749999999996\n",
      "label Moderate_Demented mistake 0.03956043956043953\n",
      "time 11.74399209022522 sec net par\n",
      "epoch  36 ,loss  0.8124612309038639 , train acc 0.9509765625\n",
      "label Non_Demented mistake 0.06395348837209303\n",
      "Epoch   141: reducing learning rate of group 0 to 7.5472e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.078125\n",
      "label Moderate_Demented mistake 0.0945054945054945\n",
      "time 11.744545459747314 sec net par\n",
      "epoch  37 ,loss  0.8044193149544299 , train acc 0.9599609375\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   146: reducing learning rate of group 0 to 7.4717e-04.\n",
      "label Mild_Demented mistake 0.10781249999999998\n",
      "label Moderate_Demented mistake 0.1384615384615384\n",
      "time 11.74981164932251 sec net par\n",
      "epoch  38 ,loss  0.8080543028190732 , train acc 0.9552734375\n",
      "label Non_Demented mistake 0.03488372093023251\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.05625000000000002\n",
      "Epoch   151: reducing learning rate of group 0 to 7.3970e-04.\n",
      "label Moderate_Demented mistake 0.0703296703296703\n",
      "time 11.750478982925415 sec net par\n",
      "epoch  39 ,loss  0.7984756748192012 , train acc 0.9673828125\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.271875\n",
      "label Moderate_Demented mistake 0.12307692307692308\n",
      "Epoch   156: reducing learning rate of group 0 to 7.3230e-04.\n",
      "time 11.732331275939941 sec net par\n",
      "epoch  40 ,loss  0.8074006326496601 , train acc 0.95859375\n",
      "label Non_Demented mistake 0.15116279069767447\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0546875\n",
      "label Moderate_Demented mistake 0.13626373626373622\n",
      "time 11.784731149673462 sec net par\n",
      "epoch  41 ,loss  0.8003957318142056 , train acc 0.9681640625\n",
      "label Non_Demented mistake 0.08139534883720934\n",
      "Epoch   161: reducing learning rate of group 0 to 7.2498e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.04531249999999998\n",
      "label Moderate_Demented mistake 0.06373626373626373\n",
      "time 11.835907697677612 sec net par\n",
      "epoch  42 ,loss  0.8011275120079517 , train acc 0.96875\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   166: reducing learning rate of group 0 to 7.1773e-04.\n",
      "label Mild_Demented mistake 0.09687500000000004\n",
      "label Moderate_Demented mistake 0.0945054945054945\n",
      "time 11.853314876556396 sec net par\n",
      "epoch  43 ,loss  0.7965457127429545 , train acc 0.971875\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.3375\n",
      "Epoch   171: reducing learning rate of group 0 to 7.1055e-04.\n",
      "label Moderate_Demented mistake 0.06813186813186811\n",
      "time 11.850735425949097 sec net par\n",
      "epoch  44 ,loss  0.7962203794158995 , train acc 0.9734375\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.08125000000000004\n",
      "label Moderate_Demented mistake 0.0703296703296703\n",
      "Epoch   176: reducing learning rate of group 0 to 7.0345e-04.\n",
      "time 11.812459230422974 sec net par\n",
      "epoch  45 ,loss  0.8021694794297218 , train acc 0.9681640625\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.09999999999999998\n",
      "label Moderate_Demented mistake 0.05054945054945059\n",
      "time 11.83051061630249 sec net par\n",
      "epoch  46 ,loss  0.8027716646902263 , train acc 0.9625\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "Epoch   181: reducing learning rate of group 0 to 6.9641e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.04062500000000002\n",
      "label Moderate_Demented mistake 0.05054945054945059\n",
      "time 11.794363737106323 sec net par\n",
      "epoch  47 ,loss  0.7961628986522555 , train acc 0.9720703125\n",
      "label Non_Demented mistake 0.08139534883720934\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   186: reducing learning rate of group 0 to 6.8945e-04.\n",
      "label Mild_Demented mistake 0.04843750000000002\n",
      "label Moderate_Demented mistake 0.0703296703296703\n",
      "time 11.797304630279541 sec net par\n",
      "epoch  48 ,loss  0.7931020660325885 , train acc 0.9767578125\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.08437499999999998\n",
      "Epoch   191: reducing learning rate of group 0 to 6.8255e-04.\n",
      "label Moderate_Demented mistake 0.10109890109890107\n",
      "time 11.823928594589233 sec net par\n",
      "epoch  49 ,loss  0.7926037651486695 , train acc 0.975390625\n",
      "label Non_Demented mistake 0.07558139534883723\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.06406250000000002\n",
      "label Moderate_Demented mistake 0.0483516483516484\n",
      "Epoch   196: reducing learning rate of group 0 to 6.7573e-04.\n",
      "time 11.814611911773682 sec net par\n",
      "epoch  50 ,loss  0.788472801912576 , train acc 0.979296875\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.16249999999999998\n",
      "label Moderate_Demented mistake 0.05494505494505497\n",
      "time 11.768559217453003 sec net par\n",
      "epoch  51 ,loss  0.7937491326592863 , train acc 0.9724609375\n",
      "label Non_Demented mistake 0.07558139534883723\n",
      "Epoch   201: reducing learning rate of group 0 to 6.6897e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.018750000000000044\n",
      "label Moderate_Demented mistake 0.08131868131868136\n",
      "time 11.764075994491577 sec net par\n",
      "epoch  52 ,loss  0.7901150826364756 , train acc 0.9798828125\n",
      "label Non_Demented mistake 0.07558139534883723\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   206: reducing learning rate of group 0 to 6.6228e-04.\n",
      "label Mild_Demented mistake 0.012499999999999956\n",
      "label Moderate_Demented mistake 0.19999999999999996\n",
      "time 11.770179510116577 sec net par\n",
      "epoch  53 ,loss  0.7955599054694176 , train acc 0.9708984375\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.12656250000000002\n",
      "Epoch   211: reducing learning rate of group 0 to 6.5566e-04.\n",
      "label Moderate_Demented mistake 0.07252747252747249\n",
      "time 11.767102003097534 sec net par\n",
      "epoch  54 ,loss  0.7910505193285644 , train acc 0.97890625\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.043749999999999956\n",
      "label Moderate_Demented mistake 0.0461538461538461\n",
      "Epoch   216: reducing learning rate of group 0 to 6.4910e-04.\n",
      "time 11.767369031906128 sec net par\n",
      "epoch  55 ,loss  0.7851002952083945 , train acc 0.9830078125\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.171875\n",
      "label Moderate_Demented mistake 0.02857142857142858\n",
      "time 11.774499654769897 sec net par\n",
      "epoch  56 ,loss  0.7886326261796057 , train acc 0.9822265625\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "Epoch   221: reducing learning rate of group 0 to 6.4261e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.035937499999999956\n",
      "label Moderate_Demented mistake 0.04175824175824172\n",
      "time 11.778619050979614 sec net par\n",
      "epoch  57 ,loss  0.7886667093262076 , train acc 0.981640625\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   226: reducing learning rate of group 0 to 6.3619e-04.\n",
      "label Mild_Demented mistake 0.09843749999999996\n",
      "label Moderate_Demented mistake 0.05274725274725278\n",
      "time 11.813908576965332 sec net par\n",
      "epoch  58 ,loss  0.7901772758923471 , train acc 0.979296875\n",
      "label Non_Demented mistake 0.22093023255813948\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.010937500000000044\n",
      "Epoch   231: reducing learning rate of group 0 to 6.2982e-04.\n",
      "label Moderate_Demented mistake 0.13626373626373622\n",
      "time 11.813884973526001 sec net par\n",
      "epoch  59 ,loss  0.8136832294985652 , train acc 0.9580078125\n",
      "label Non_Demented mistake 0.07558139534883723\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.06406250000000002\n",
      "label Moderate_Demented mistake 0.04395604395604391\n",
      "Epoch   236: reducing learning rate of group 0 to 6.2353e-04.\n",
      "time 11.77475905418396 sec net par\n",
      "epoch  60 ,loss  0.7936162268742919 , train acc 0.97890625\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.042187500000000044\n",
      "label Moderate_Demented mistake 0.06153846153846154\n",
      "time 11.749542474746704 sec net par\n",
      "epoch  61 ,loss  0.7920873961411417 , train acc 0.97890625\n",
      "label Non_Demented mistake 0.06395348837209303\n",
      "Epoch   241: reducing learning rate of group 0 to 6.1729e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.014062499999999978\n",
      "label Moderate_Demented mistake 0.06593406593406592\n",
      "time 11.746785640716553 sec net par\n",
      "epoch  62 ,loss  0.7894452302716672 , train acc 0.98203125\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   246: reducing learning rate of group 0 to 6.1112e-04.\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.03736263736263734\n",
      "time 11.742295265197754 sec net par\n",
      "epoch  63 ,loss  0.7871772525832057 , train acc 0.9845703125\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03125\n",
      "Epoch   251: reducing learning rate of group 0 to 6.0501e-04.\n",
      "label Moderate_Demented mistake 0.0703296703296703\n",
      "time 11.745283842086792 sec net par\n",
      "epoch  64 ,loss  0.7879222980700433 , train acc 0.9837890625\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.06562500000000004\n",
      "label Moderate_Demented mistake 0.06373626373626373\n",
      "Epoch   256: reducing learning rate of group 0 to 5.9896e-04.\n",
      "time 11.745587348937988 sec net par\n",
      "epoch  65 ,loss  0.7877206676639616 , train acc 0.9833984375\n",
      "label Non_Demented mistake 0.05232558139534882\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.0461538461538461\n",
      "time 11.74409294128418 sec net par\n",
      "epoch  66 ,loss  0.7874931339174509 , train acc 0.981640625\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "Epoch   261: reducing learning rate of group 0 to 5.9297e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.020312499999999956\n",
      "label Moderate_Demented mistake 0.04175824175824172\n",
      "time 11.740632057189941 sec net par\n",
      "epoch  67 ,loss  0.7899311613291502 , train acc 0.98125\n",
      "label Non_Demented mistake 0.31976744186046513\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   266: reducing learning rate of group 0 to 5.8704e-04.\n",
      "label Mild_Demented mistake 0.014062499999999978\n",
      "label Moderate_Demented mistake 0.4087912087912088\n",
      "time 11.749478816986084 sec net par\n",
      "epoch  68 ,loss  0.788712705951184 , train acc 0.9806640625\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.009375000000000022\n",
      "Epoch   271: reducing learning rate of group 0 to 5.8117e-04.\n",
      "label Moderate_Demented mistake 0.08791208791208793\n",
      "time 11.751716136932373 sec net par\n",
      "epoch  69 ,loss  0.7870079190470278 , train acc 0.9830078125\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.02197802197802201\n",
      "Epoch   276: reducing learning rate of group 0 to 5.7535e-04.\n",
      "time 11.763505697250366 sec net par\n",
      "epoch  70 ,loss  0.7869096715003252 , train acc 0.985546875\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.012499999999999956\n",
      "label Moderate_Demented mistake 0.03956043956043953\n",
      "time 11.750483751296997 sec net par\n",
      "epoch  71 ,loss  0.7846187995746732 , train acc 0.9861328125\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "Epoch   281: reducing learning rate of group 0 to 5.6960e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.034375000000000044\n",
      "label Moderate_Demented mistake 0.04395604395604391\n",
      "time 11.756164789199829 sec net par\n",
      "epoch  72 ,loss  0.7857802761718631 , train acc 0.9857421875\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   286: reducing learning rate of group 0 to 5.6391e-04.\n",
      "label Mild_Demented mistake 0.028124999999999956\n",
      "label Moderate_Demented mistake 0.02857142857142858\n",
      "time 11.75202751159668 sec net par\n",
      "epoch  73 ,loss  0.7856642310507596 , train acc 0.9849609375\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.06718749999999996\n",
      "Epoch   291: reducing learning rate of group 0 to 5.5827e-04.\n",
      "label Moderate_Demented mistake 0.03516483516483515\n",
      "time 11.75416612625122 sec net par\n",
      "epoch  74 ,loss  0.7883900278247893 , train acc 0.9830078125\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03281250000000002\n",
      "label Moderate_Demented mistake 0.03956043956043953\n",
      "Epoch   296: reducing learning rate of group 0 to 5.5268e-04.\n",
      "time 11.743402242660522 sec net par\n",
      "epoch  75 ,loss  0.789386032614857 , train acc 0.9833984375\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.09062499999999996\n",
      "label Moderate_Demented mistake 0.03736263736263734\n",
      "time 11.749590396881104 sec net par\n",
      "epoch  76 ,loss  0.785816905554384 , train acc 0.9859375\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "Epoch   301: reducing learning rate of group 0 to 5.4716e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.020312499999999956\n",
      "label Moderate_Demented mistake 0.03516483516483515\n",
      "time 11.74518895149231 sec net par\n",
      "epoch  77 ,loss  0.7878116327337921 , train acc 0.982421875\n",
      "label Non_Demented mistake 0.05232558139534882\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   306: reducing learning rate of group 0 to 5.4169e-04.\n",
      "label Mild_Demented mistake 0.014062499999999978\n",
      "label Moderate_Demented mistake 0.02857142857142858\n",
      "time 11.746689081192017 sec net par\n",
      "epoch  78 ,loss  0.7948406632058322 , train acc 0.9755859375\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.029687499999999978\n",
      "Epoch   311: reducing learning rate of group 0 to 5.3627e-04.\n",
      "label Moderate_Demented mistake 0.03516483516483515\n",
      "time 11.74201774597168 sec net par\n",
      "epoch  79 ,loss  0.7861642404459417 , train acc 0.98125\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03125\n",
      "label Moderate_Demented mistake 0.05494505494505497\n",
      "Epoch   316: reducing learning rate of group 0 to 5.3091e-04.\n",
      "time 11.743399143218994 sec net par\n",
      "epoch  80 ,loss  0.7868621712550521 , train acc 0.98359375\n",
      "label Non_Demented mistake 0.011627906976744207\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03749999999999998\n",
      "label Moderate_Demented mistake 0.05054945054945059\n",
      "time 11.754407167434692 sec net par\n",
      "epoch  81 ,loss  0.7866517673246562 , train acc 0.9873046875\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "Epoch   321: reducing learning rate of group 0 to 5.2560e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.015625\n",
      "label Moderate_Demented mistake 0.0241758241758242\n",
      "time 11.746713161468506 sec net par\n",
      "epoch  82 ,loss  0.7852950803935528 , train acc 0.986328125\n",
      "label Non_Demented mistake 0.07558139534883723\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   326: reducing learning rate of group 0 to 5.2034e-04.\n",
      "label Mild_Demented mistake 0.020312499999999956\n",
      "label Moderate_Demented mistake 0.12747252747252746\n",
      "time 11.75134563446045 sec net par\n",
      "epoch  83 ,loss  0.7864650376141071 , train acc 0.9861328125\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.015625\n",
      "Epoch   331: reducing learning rate of group 0 to 5.1514e-04.\n",
      "label Moderate_Demented mistake 0.02637362637362639\n",
      "time 11.743884086608887 sec net par\n",
      "epoch  84 ,loss  0.7862800625152886 , train acc 0.9865234375\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.026562500000000044\n",
      "label Moderate_Demented mistake 0.01318681318681314\n",
      "Epoch   336: reducing learning rate of group 0 to 5.0999e-04.\n",
      "time 11.73958969116211 sec net par\n",
      "epoch  85 ,loss  0.7865154077298939 , train acc 0.984765625\n",
      "label Non_Demented mistake 0.12209302325581395\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.018750000000000044\n",
      "label Moderate_Demented mistake 0.0461538461538461\n",
      "time 11.739579916000366 sec net par\n",
      "epoch  86 ,loss  0.7862997888587415 , train acc 0.9859375\n",
      "label Non_Demented mistake 0.05813953488372092\n",
      "Epoch   341: reducing learning rate of group 0 to 5.0489e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.026562500000000044\n",
      "label Moderate_Demented mistake 0.03516483516483515\n",
      "time 11.746649026870728 sec net par\n",
      "epoch  87 ,loss  0.7859631758183241 , train acc 0.9869140625\n",
      "label Non_Demented mistake 0.06976744186046513\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   346: reducing learning rate of group 0 to 4.9984e-04.\n",
      "label Mild_Demented mistake 0.029687499999999978\n",
      "label Moderate_Demented mistake 0.01318681318681314\n",
      "time 11.764963150024414 sec net par\n",
      "epoch  88 ,loss  0.7874214225448668 , train acc 0.983203125\n",
      "label Non_Demented mistake 0.03488372093023251\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.028124999999999956\n",
      "Epoch   351: reducing learning rate of group 0 to 4.9484e-04.\n",
      "label Moderate_Demented mistake 0.06813186813186811\n",
      "time 11.752234697341919 sec net par\n",
      "epoch  89 ,loss  0.7893555997870862 , train acc 0.977734375\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03281250000000002\n",
      "label Moderate_Demented mistake 0.03296703296703296\n",
      "Epoch   356: reducing learning rate of group 0 to 4.8989e-04.\n",
      "time 11.74368691444397 sec net par\n",
      "epoch  90 ,loss  0.7854530485346913 , train acc 0.9865234375\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.009375000000000022\n",
      "label Moderate_Demented mistake 0.01758241758241763\n",
      "time 11.752079010009766 sec net par\n",
      "epoch  91 ,loss  0.7824916653335094 , train acc 0.9892578125\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "Epoch   361: reducing learning rate of group 0 to 4.8499e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.012499999999999956\n",
      "label Moderate_Demented mistake 0.01758241758241763\n",
      "time 11.74804162979126 sec net par\n",
      "epoch  92 ,loss  0.784478803165257 , train acc 0.988671875\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   366: reducing learning rate of group 0 to 4.8014e-04.\n",
      "label Mild_Demented mistake 0.025000000000000022\n",
      "label Moderate_Demented mistake 0.01978021978021982\n",
      "time 11.74668002128601 sec net par\n",
      "epoch  93 ,loss  0.7845739512704313 , train acc 0.9853515625\n",
      "label Non_Demented mistake 0.05232558139534882\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.010937500000000044\n",
      "Epoch   371: reducing learning rate of group 0 to 4.7534e-04.\n",
      "label Moderate_Demented mistake 0.05054945054945059\n",
      "time 11.751888990402222 sec net par\n",
      "epoch  94 ,loss  0.782303289975971 , train acc 0.987109375\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.034375000000000044\n",
      "label Moderate_Demented mistake 0.00659340659340657\n",
      "Epoch   376: reducing learning rate of group 0 to 4.7059e-04.\n",
      "time 11.748382329940796 sec net par\n",
      "epoch  95 ,loss  0.7851969595067203 , train acc 0.9857421875\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.042187500000000044\n",
      "label Moderate_Demented mistake 0.01758241758241763\n",
      "time 11.743651866912842 sec net par\n",
      "epoch  96 ,loss  0.7848587660118937 , train acc 0.9875\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "Epoch   381: reducing learning rate of group 0 to 4.6588e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.22343749999999996\n",
      "label Moderate_Demented mistake 0.14945054945054947\n",
      "time 11.757977962493896 sec net par\n",
      "epoch  97 ,loss  0.7895931405946612 , train acc 0.981640625\n",
      "label Non_Demented mistake 0.09302325581395354\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   386: reducing learning rate of group 0 to 4.6122e-04.\n",
      "label Mild_Demented mistake 0.0\n",
      "label Moderate_Demented mistake 0.1450549450549451\n",
      "time 11.752943754196167 sec net par\n",
      "epoch  98 ,loss  0.7825767272152007 , train acc 0.988671875\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.029687499999999978\n",
      "Epoch   391: reducing learning rate of group 0 to 4.5661e-04.\n",
      "label Moderate_Demented mistake 0.01098901098901095\n",
      "time 11.744241714477539 sec net par\n",
      "epoch  99 ,loss  0.7833367357961833 , train acc 0.988671875\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.020312499999999956\n",
      "label Moderate_Demented mistake 0.01098901098901095\n",
      "Epoch   396: reducing learning rate of group 0 to 4.5204e-04.\n",
      "time 11.784675598144531 sec net par\n",
      "epoch  100 ,loss  0.7835460565984249 , train acc 0.987109375\n",
      "label Non_Demented mistake 0.04069767441860461\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.009375000000000022\n",
      "label Moderate_Demented mistake 0.06153846153846154\n",
      "time 11.840978622436523 sec net par\n",
      "epoch  101 ,loss  0.786801278591156 , train acc 0.9859375\n",
      "label Non_Demented mistake 0.029069767441860517\n",
      "Epoch   401: reducing learning rate of group 0 to 4.4752e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.0234375\n",
      "label Moderate_Demented mistake 0.01758241758241763\n",
      "time 11.746888637542725 sec net par\n",
      "epoch  102 ,loss  0.7860237811692059 , train acc 0.9861328125\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   406: reducing learning rate of group 0 to 4.4305e-04.\n",
      "label Mild_Demented mistake 0.025000000000000022\n",
      "label Moderate_Demented mistake 0.03076923076923077\n",
      "time 11.742932558059692 sec net par\n",
      "epoch  103 ,loss  0.7853465792723 , train acc 0.9880859375\n",
      "label Non_Demented mistake 0.005813953488372103\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.012499999999999956\n",
      "Epoch   411: reducing learning rate of group 0 to 4.3862e-04.\n",
      "label Moderate_Demented mistake 0.03956043956043953\n",
      "time 11.756571054458618 sec net par\n",
      "epoch  104 ,loss  0.7835967261344194 , train acc 0.9890625\n",
      "label Non_Demented mistake 0.01744186046511631\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.028124999999999956\n",
      "label Moderate_Demented mistake 0.0241758241758242\n",
      "Epoch   416: reducing learning rate of group 0 to 4.3423e-04.\n",
      "time 11.879554986953735 sec net par\n",
      "epoch  105 ,loss  0.7869084626436234 , train acc 0.9873046875\n",
      "label Non_Demented mistake 0.03488372093023251\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.017187500000000022\n",
      "label Moderate_Demented mistake 0.04175824175824172\n",
      "time 11.891826391220093 sec net par\n",
      "epoch  106 ,loss  0.7978547415696084 , train acc 0.9720703125\n",
      "label Non_Demented mistake 0.046511627906976716\n",
      "Epoch   421: reducing learning rate of group 0 to 4.2989e-04.\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.028124999999999956\n",
      "label Moderate_Demented mistake 0.0241758241758242\n",
      "time 11.839571475982666 sec net par\n",
      "epoch  107 ,loss  0.7860157173126936 , train acc 0.9865234375\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "Epoch   426: reducing learning rate of group 0 to 4.2559e-04.\n",
      "label Mild_Demented mistake 0.020312499999999956\n",
      "label Moderate_Demented mistake 0.01538461538461533\n",
      "time 11.836869239807129 sec net par\n",
      "epoch  108 ,loss  0.7837925972416997 , train acc 0.9876953125\n",
      "label Non_Demented mistake 0.023255813953488413\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.015625\n",
      "Epoch   431: reducing learning rate of group 0 to 4.2133e-04.\n",
      "label Moderate_Demented mistake 0.01758241758241763\n",
      "time 11.817862272262573 sec net par\n",
      "epoch  109 ,loss  0.7835779339075089 , train acc 0.989453125\n",
      "label Non_Demented mistake 0.0\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.012499999999999956\n",
      "label Moderate_Demented mistake 0.03736263736263734\n",
      "Epoch   436: reducing learning rate of group 0 to 4.1712e-04.\n",
      "time 11.825688600540161 sec net par\n",
      "epoch  110 ,loss  0.7866566870361567 , train acc 0.9859375\n",
      "label Non_Demented mistake 0.06395348837209303\n",
      "label Very_Mild_Demented mistake 1.0\n",
      "label Mild_Demented mistake 0.03281250000000002\n",
      "label Moderate_Demented mistake 0.02637362637362639\n",
      "time 11.7873055934906 sec net par\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c4c568f65875>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m )\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c4c568f65875>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_data, train_labels, test_data, test_labels, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, strider=1, use_1x1conv=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strider)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strider) if use_1x1conv else None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels) if use_1x1conv else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x = self.bn3(self.conv3(x))\n",
    "        y += x\n",
    "        return F.relu(y)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.res = nn.Sequential(\n",
    "            *self.resnet_block(64, 64, 2, first_block=True),\n",
    "            *self.resnet_block(64, 128, 2),\n",
    "            *self.resnet_block(128, 256, 2),\n",
    "            *self.resnet_block(256, 512, 2)\n",
    "        )\n",
    "        self.ft = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def resnet_block(self, input_channels, num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(input_channels, num_channels, strider=2, use_1x1conv=True))\n",
    "            else:\n",
    "                blk.append(Residual(input_channels, num_channels, strider=1))\n",
    "            input_channels = num_channels\n",
    "        return blk\n",
    "\n",
    "    def forward(self, img):\n",
    "        y = self.conv(img)\n",
    "        y = self.res(y)\n",
    "        #print(\"Conv output shape: \", y.shape)  # 打印卷積層輸出的形狀\n",
    "\n",
    "        y = self.ft(y)\n",
    "        return y\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in data_iter:\n",
    "        if isinstance(net, torch.nn.Module):\n",
    "            net.eval()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            acc_sum += (net(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            net.train()\n",
    "        else:\n",
    "            if ('is_training' in net.__code__.co_varnames):\n",
    "                acc_sum += (net(x, is_training=False).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "            else:\n",
    "                acc_sum += (net(x).argmax(dim=1) == y).float().sum().cpu().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(net, train_data, train_labels, test_data, test_labels, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    mistake_dict = {name: [] for name in label_to_name.values()}\n",
    "    ls=[]\n",
    "\n",
    "# 學習率調度器\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=4, factor=0.99, verbose=True)\n",
    "\n",
    "\n",
    "    a=1/grouped_test_data[0].size(0)\n",
    "    b=1/grouped_test_data[1].size(0)\n",
    "    c=1/grouped_test_data[2].size(0)\n",
    "    d=1/grouped_test_data[3].size(0)\n",
    "    weights = torch.tensor([a/a+b+c+d, b/a+b+c+d, c/a+b+c+d,d/a+b+c+d]).to('cuda')   # 這裡的數字可以根據需要調整\n",
    "\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count = 0.0, 0.0, 0, 0\n",
    "        start = time.time()\n",
    "        dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "        train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "        for data, labels in train_iter:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_hat = net(data)\n",
    "            l = loss(y_hat, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().cpu().item()\n",
    "\n",
    "            n += labels.shape[0]\n",
    "            batch_count += 1\n",
    "            ls.append(train_l_sum/batch_count)\n",
    "\n",
    "            '''\n",
    "            # 打印每个batch后的梯度信息\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f'{name} grad: {param.grad.mean()}')\n",
    "            '''\n",
    "\n",
    "        print('epoch ',epoch+1,',loss ',train_l_sum/batch_count,', train acc',train_acc_sum/n)\n",
    "        # 遍歷每個label\n",
    "        for label in grouped_test_data.keys():\n",
    "            # 獲取對應的 data 和 label\n",
    "            data = grouped_test_data[label]\n",
    "            labels = torch.tensor([label] * len(data), dtype=torch.long)  # 為每個樣本生成相應的 label\n",
    "\n",
    "            # 創建 TensorDataset 和 DataLoader\n",
    "            dataset = TensorDataset(data, labels)\n",
    "            data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # 計算該label的準確度\n",
    "            test_acc = evaluate_accuracy(data_loader, net)\n",
    "            # 使用label_to_name將label轉換為名稱，並將test_acc儲存到字典中\n",
    "            name = label_to_name[label]\n",
    "            mistake_dict[name].append(1-test_acc)\n",
    "            print('label',name,\"mistake\",1-test_acc)\n",
    "            #學習率\n",
    "            scheduler.step(train_acc_sum/n)\n",
    "        print('time',time.time()-start,'sec','net par')\n",
    "    x=np.linspace(0,num_epochs,num=num_epochs)\n",
    "    for label in grouped_test_data.keys():\n",
    "        name = label_to_name[label]  # 将label转换为对应的名称\n",
    "        plt.plot(x, mistake_dict[name], label=name)    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('test_mistake')\n",
    "    plt.yscale('log')          # log y-axis\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    x_2=np.linspace(0,num_epochs,len(ls))\n",
    "    plt.plot(x_2,ls)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "lr, num_epochs = 0.01, 1000\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = ResNet()\n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "toptimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "train(net, train_data=train_data, train_labels=train_labels, test_data=test_data, test_labels=test_labels, batch_size=40, optimizer=toptimizer, device='cuda', num_epochs=num_epochs)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.9)  # 例如，設定為最多使用 90% 的顯存\n",
    "torch.backends.cudnn.benchmark = True  # 啟用 cudnn 的最佳化\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
